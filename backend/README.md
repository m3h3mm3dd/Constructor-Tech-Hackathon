# EdTech / LMS Competitor Intelligence Agent

This repository contains a **FastAPI** backend for automating structured
competitor research in the education technology and learning management
system (LMS) markets. The system senses the market by discovering
relevant companies, decides by synthesising information from public web
sources into consistent profiles and statistics, and acts by storing
data, exposing rich APIs and exporting results.

## Features

### üîç Automated discovery and profiling

- **Start a research job** ‚Äì `POST /api/v1/research/start` begins a new
  investigation for a market segment (e.g. "university LMS platforms in
  Europe"). The system calls a language model to propose up to
  `max_companies` relevant companies, upserts them into the database
  and runs the profiling pipeline for each.

- **Check job status** ‚Äì `GET /api/v1/research/{job_id}` returns the
  status of a research job along with the companies discovered and
  whether they have been profiled.

- **List companies** ‚Äì `GET /api/v1/research/companies` lists all
  stored companies with optional filters such as segment, category,
  region and the presence of AI features. Useful for tables or
  dropdowns.

- **Retrieve a profile** ‚Äì `GET /api/v1/research/companies/{id}` returns
  a full wiki‚Äëstyle profile for a single company along with the
  underlying source documents.

- **Refresh a profile** ‚Äì `POST /api/v1/research/companies/{id}/refresh`
  re‚Äëruns the profiling pipeline to update a company with the latest
  information from the web.

- **Compare companies** ‚Äì `POST /api/v1/research/companies/compare`
  accepts a list of company IDs and returns common strengths,
  differentiators and opportunity gaps generated by the language model.

- **Statistics overview** ‚Äì `GET /api/v1/research/stats/overview`
  aggregates the company dataset into buckets (by category, region,
  pricing model and AI features) ready for visualisation.

- **Export dataset** ‚Äì `GET /api/v1/research/export` downloads a CSV of
  all companies and their core attributes for offline analysis.

### üß† Agent architecture

- **Sense** ‚Äì The discovery pipeline asks the language model to
  recommend companies for a segment. The profiling pipeline uses a
  configurable search API to fetch relevant web pages and the
  company's own site, extracts plain text and stores the documents for
  provenance.

- **Decide** ‚Äì Using the aggregated context, the language model
  produces a structured JSON profile covering category, region, size
  bucket, products, pricing model, strengths, risks, market position,
  AI features and compliance tags. Profiles are normalised via
  Pydantic models.

- **Act** ‚Äì Profiles and their sources are persisted to a SQLite (or
  any SQLAlchemy) database. Aggregated statistics and comparisons are
  computed on demand. A simple export endpoint produces CSVs for
  integration into existing workflows.

## Quickstart

1. **Install dependencies**

   ```bash
   cd backend
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Create a `.env` file** in `backend/` with at least the following
   variables:

   ```env
   OPENAI_API_KEY=<your OpenRouter API key>
   LLM_MODEL=openai/gpt-4.1-mini  # default model; override as desired
   SEARCH_API_KEY=<your search API key>
   SEARCH_API_BASE_URL=https://api.serper.dev  # or another provider
   DB_URL=sqlite+aiosqlite:///./test.db
   CORS_ORIGINS=*
   ```

3. **Initialise the database** (optional). For a fresh SQLite file this
   happens automatically on first run. If you wish to seed the DB with
   your own data, create a script under `scripts/`.

4. **Run the server**

   ```bash
   uvicorn app.main:app --reload
   ```

5. **Explore the API**

   - Swagger UI: <http://localhost:8000/docs>
   - ReDoc: <http://localhost:8000/redoc>

   All endpoints are prefixed with `/api/v1`. For example, to start a
   research job, send a POST request to `/api/v1/research/start` with a
   JSON body:

   ```json
   {
     "segment": "university LMS platforms in Europe",
     "max_companies": 15
   }
   ```

## Architecture overview

- **`app/db/models.py`** ‚Äì SQLAlchemy ORM models: `Company`,
  `SourceDocument` and `ResearchJob` store profiles, source documents
  and job metadata. Existing `User`, `ChatSession` and `Message` models
  are retained for potential future extensions.

- **`app/schemas/research.py`** ‚Äì Pydantic schemas defining request and
  response payloads for the research endpoints.

- **`app/services/research_service.py`** ‚Äì Core business logic
  implementing discovery, profiling, refreshing, statistics and
  comparison pipelines. External web searches are performed via a
  pluggable search client (see `app/core/search.py`). Calls to the
  language model use the OpenAI SDK pointed at OpenRouter and are
  configurable via environment variables.

- **`app/api/v1/research_routes.py`** ‚Äì FastAPI router exposing REST
  endpoints for starting and monitoring jobs, listing and retrieving
  companies, refreshing profiles, comparing companies, aggregating
  statistics and exporting data.

- **`app/core/config.py`** ‚Äì Centralised configuration loaded from the
  `.env` file. Defines keys for the database, language model and
  search provider.

Feel free to extend the service with additional endpoints or connect
front‚Äëend visualisations. The code is organised into discrete layers
to make it easy to maintain and adapt to different hackathon tracks.